#! Sequence 2 Sequenct Learning
#FrameWork

# X = [x1,x2,x3,x4,x5]
# Y = [y1,y2,y3,y4,y5]


# N to 1 => ex) 감성분석
# N to M => ex) 기계번역

# 음성인식기 : 음성 -> 텍스트
# 동영상검색 : 비디오프레임들 -> 프레임 캡션
# 품사태깅 : 문장 -> 단어별 품사
# 산수문제 : 계산식(19+3x2) -> 결과(25)
# 기계번역 : 한국어 문장 -> 영어 문장
# 문장완성기 : 문장 시작 부분 -> 문장 뒷 부분
# 대화모델 : 입력 문장 -> 그에 대응하는 대답
#------------------------------------------------------------------
#! Attention
#복수개의 데이터에서 무엇인가 가치가 있는 컴팩트한 데이터로 축소하는 과정

#NOTE :Blending 비유
#Sequence의 정보를 섞어 보는 것은 어떨까?

#액체 Q와 시료들 [I1, I2, I3, I4, I5]
#관심있는 Q와 가장 반응성이 높을 시료들을 조합해서 새로운 물질 만드는것이 목표

#idea1
#일단 Q와 각 시료들을 다 섞어보자. 
#각 시료별 반응성 확인
#반응성이 높은 시료들끼리 반응성 크기 별 비율로 섞어 새로운 물질 만듬

#Query Vector -> [Multiple Vectors] -> blended Vector 얻기

#NOTE :Query가 없는 경우
#[x1],[x2],[x3] 각 벡터별 중요별로 곱한다
#[x'1],[x'2],[x'3]
#새로운 벡터들을 다 더한다 -> [x'1 + x'2 +x'3] *하나의 벡터만 나옴
#*중요도를 Attention Score라고 부름

#NOTE :중요도 구하기
#메트릭스 혹은 네트워크를 통해 각 벡터를 하나의 값으로 나오도록 한다. (메트릭스 혹은 네트워크는 벡터별 공유함)
#각벡터별 나온 값 : [a1], [a2], [a3]
#소프트맥스 함수를 통해 각 값별 비율을 구한다.
#Attention Score : [e^(a1)/[e^(a1)+e^(a2)+e^(a3)]], [e^(a1)/[e^(a1)+e^(a2)+e^(a3)]], [e^(a1)/[e^(a1)+e^(a2)+e^(a3)]] 

#NOTE : 블렌드 벡터 구하기
#각 벡터별 자신의 어텐션 스코어 값과 곱한후 다 더한다.
#Xs = [x1*s1 + x2*s2 + x3*x3] *어텐션 벡터(여러 데이터가 중요에 따라 축약된 하나의 벡터)

#---------------------------
#NOTE : Q(쿼리)가 있는 경우
#입력 데이터 [x1], [x2], [x3]
#Q(쿼리)와 가장 잘 반응하는 입력 데이터를 모아 블렌딩

#각 입력 벡터별 하나의 값 만들기
#[a1],[a2],[a3] 메트릭스 혹은 네트워크를 통해 Query와 입력벡터 하나로 스칼라값 도출
#[s1],[s2],[s3] 어텐션 스코어 : 각 값별로 소프트 맥스를 통해 중요도 비율 구하기
#[s1*x1], [s2*x2], [s3*x3] 각 스코어와 입력벡터를 곱한후 더하면 Blended 벡터 완성
#Xs = [s1*x1 + s2*x2 + s3*x3]

#NOTE : 쿼리를 어떻게 입력데이터와 반응성을 얻을 수 있을까?
#(1) Addictive Approach : 덧셈을 이용하자
#(2) Multiplicative Approach : 곱셈을 이용하자.

#(1) Addictive Approach
#Q + x1를 더해 새로운 벡터 B를 만들고 B를 메트릭스 또는 네트워크를 통해 하나의 스칼라값(a1)으로 도출
#Q+x1을 더하기 위해서는 Shape를 맞춰야 한다.
#Q[Q,1]의 쉐이프 조정 W[D,Q]행렬  => W[D,Q] * Q[Q,1] = Q'[D,1]
#x1[H,1]의 쉐이프 조정 U[D,H]행렬 => U[D,H] * x1[H,1] = x'1[D,1]
#B1[D,1] = Q' + x'1 
#B1을 스칼라 값으로 만들기 위한 메트릭스 혹은 네트워크 Vt *t는 전치행렬
#Vt[1,D] * B1[D,1] => a1(스칼라)

#각 입력데이터 한꺼번에 처리
#입력 3개의 데이터 묶음[3,D] * V[D,1] => [a1, a2, a3] 

#(2) Multiplicative Approach : 트랜스 포머 모델에서 많이 사용
#(2-1) Dot
#Q[1,H] * x1[H,1] => a1[스칼라]     *Q,x는 Shape이 같아야함
#각 입력 데이터 한번에 처리
#X[3,H] * Q[H,1] => S[3] 스칼라

#(2-2) General
#Q의 Shape가 x와 다른경우
#W[H,Q] * Q[Q,1] = Q'[H,1]
#X[3,H] * Q'[H,1] => S[3] 스칼라

#(2-3) Concat
#Q[Q,1]벡터와 x1[H,1]벡터를 이어 붙인다. 
#b1[Q+H, 1] = b1[D, 1]
#Vt[1,D]를 통해 스칼라값 생성
#Vt[1,D]*b1[D,1] => a1

#한번에 처리
#B[3,D] * v[D,1] => A[3]
#------------------------------------------------------------------------------------------
#예시
#서울역 근처 스타벅스 로 가자 -> Let's go to Starbucks near Seoul Station
#Let's go 는 가자에 Attention을 줘야함

#Attendion 메커니즘 = 
#Global(시퀀스인 입력 데이터 순서가 중요하지 않음), 
#Selective(어떤 입력 데이터가 중요한지 선별), 
#Dynamic(decoding 즉, 출력 하나 이루어질때마다 새롭게 계산)






